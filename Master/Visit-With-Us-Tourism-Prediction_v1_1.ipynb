{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["K7HT7f_W63Q9","gaE_eACI7Wdl","2XxU9ZdG-DE9","vpSiNqjnWIf_"],"mount_file_id":"11jhgihTvfEl_lQBJdTK_BRuURY6lfv_Q","authorship_tag":"ABX9TyMHwHVD5Z7GAdrmhvAAoDni"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Project Folder Structure"],"metadata":{"id":"K7HT7f_W63Q9"}},{"cell_type":"markdown","source":["Project Folder Structure\n","\n","|->`Drive`/VisitWithUs-Tourism\n","\n","  |->Master\n","  \n","    |->Data\n","        |->tourism.csv\n","        |->train.csv\n","        |->test.csv\n","    |->Deployment\n","        |->app.py\n","        |->Dockerfile\n","        |->README.md\n","        |->requirements.txt\n","    |->Model_Building\n","        |->BuildingModels.py\n","        |->DataPrepration.py\n","        |->DataRegistration.py\n","    |->Model_Dump_JOBLIB\n","        |->best_threshold.txt\n","        |->BestModel_XGBoostingClassifier.joblib\n","    |->pipeline.yml\n","  |->AIML_MLOPS_v1_1.ipynb"],"metadata":{"id":"FMxBY8Nqd0ve"}},{"cell_type":"markdown","source":["# Loading Packages"],"metadata":{"id":"gaE_eACI7Wdl"}},{"cell_type":"code","source":["!pip install huggingface_hub\n","!pip install python-dotenv\n","!pip install datasets\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install xgboost\n","!pip install seaborn\n","!pip install matplotlib\n","!pip install joblib\n","!pip install stramlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zzL9Lcv7HHX","outputId":"1548a24b-fe97-40f3-8a61-518bc4c8e93e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"]}]},{"cell_type":"markdown","source":["# MOUNTING DRIVE"],"metadata":{"id":"ognKAWMS8G_j"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1/Master/'\n","# base_path = 'VisitWithUs-Tourism_version_1_1/Master/'\n","# print(f\"Base Path {base_path}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vw9tngctf_m5","executionInfo":{"status":"ok","timestamp":1755274141968,"user_tz":-330,"elapsed":1592,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"890ba0e8-e192-41c3-926c-f32620040c59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1/Master\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","hf_token = userdata.get('HF_TOKEN')"],"metadata":{"id":"aWq8odPlk7Jk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. DATA REGISTRATION"],"metadata":{"id":"UQ_4lbVH0fTI"}},{"cell_type":"code","source":["# @title DataRegistration Class\n","%%writefile DataRegistration.py\n","import os\n","import traceback\n","import inspect\n","from huggingface_hub import HfApi, create_repo,login,hf_hub_download\n","\n","class DataRegistration:\n","  def __init__(self,base_path,hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.repoID = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.Subfolders = os.path.join(base_path,'Data')\n","    self.folder_Master = base_path\n","    self.folder_data = os.path.join(base_path,\"Data\")\n","    self.hf_token = hf_token\n","    print(f\"self.hf_token: {self.hf_token}\")\n","    os.makedirs(self.folder_data, exist_ok=True)\n","    print(f\"self.Subfolders: {self.Subfolders}\")\n","    print(f\"self.folder_Master: {self.folder_Master}\")\n","    print(f\"folder_data: {self.folder_data}\")\n","\n","  def HFCreateRepo(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      api = HfApi(token=self.hf_token)\n","      create_repo(repo_id=self.repoID,\n","                  private=False,\n","                  repo_type='dataset',\n","                  exist_ok=True)\n","      print(f\"Repo {self.repoID} created\")\n","      return True\n","\n","    except Exception as ex:\n","      if hasattr(ex,'response') and ex.response.status_code == 409:\n","        print(f\"Repo {self.repoID} already exists\")\n","        return True\n","      else:\n","        print(f\"Exception {ex}\")\n","        traceback.print_exc()\n","        return False\n","    finally:\n","      print(\"-\"*100)\n","\n","\n","  def UploadingSourceData(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      source_data_file = os.path.join(self.folder_data,'tourism.csv')\n","      print(f\"Soruce Data File {source_data_file}\")\n","      if not os.path.exists(source_data_file):\n","        raise FileNotFoundError(f\"File {source_data_file} not found\")\n","      api = HfApi()\n","      api.upload_file(\n","          path_or_fileobj = source_data_file,\n","          path_in_repo = 'Master/Data/tourism.csv',\n","          repo_id = self.repoID,\n","          repo_type='dataset',\n","          token=self.hf_token)\n","      print(f\"Source data tourism.csv uploaded into {self.repoID}\")\n","      return True\n","\n","    except Exception as ex:\n","       print(f\"Exception at {inspect.currentframe().f_code.co_name} Exception: {ex}\")\n","       traceback.print_exc()\n","       return False\n","    finally:\n","      print(\"-\"*100)\n","\n","  def ToRunPipeline(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    if not self.HFCreateRepo():\n","      print('Exception in data registration HFCreateRepo')\n","      return False\n","    else:\n","      if not self.UploadingSourceData():\n","        print('Exception in data registration UploadingSourceData')\n","        return False\n","      else:\n","        print('Data Registration Completed')\n","        return True"],"metadata":{"id":"nDUqHrVl0q7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Data Prepration"],"metadata":{"id":"2XxU9ZdG-DE9"}},{"cell_type":"code","source":["%%writefile Master/DataPrepration.py\n","import os\n","import pandas as pd\n","import inspect\n","import traceback\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","from huggingface_hub import HfApi, create_repo, login, hf_hub_download\n","\n","class DataPrepration:\n","  def __init__(self,base_path, hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.repoID = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.Subfolders = os.path.join(base_path, 'Data')\n","    self.hf_token = hf_token\n","\n","  def LoadDatasetFromHF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      df_dataset = pd.read_csv(hf_hub_download(\n","                                repo_id = self.repoID,\n","                                filename = 'Master/Data/tourism.csv',\n","                                repo_type='dataset'\n","                              ))\n","\n","      print(f'Shape of the original dataset {df_dataset.shape}')\n","\n","      if 'Unnamed: 0' in df_dataset.columns:\n","        df_dataset = df_dataset.drop(['Unnamed: 0'],axis=1)\n","\n","      print(f\"Dataset loaded from {self.repoID}/{self.Subfolders}\")\n","      print(f\"Shape of the Original Dataset: {df_dataset.shape}\")\n","      return df_dataset\n","    except Exception as ex:\n","      print(f\"Exception {ex}\")\n","      traceback.print_exc()\n","      return None\n","    finally:\n","      print('-'*50)\n","\n","  def TrainTestSplit(self,df_dataset):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      print(f\"Value Count {df_dataset['ProdTaken'].value_counts()}\")\n","\n","      df_train,df_test = train_test_split(df_dataset,\n","                                          test_size=0.2,\n","                                          random_state=42,\n","                                          stratify=df_dataset['ProdTaken'],\n","                                          shuffle=True)\n","\n","      print(f\"Shape of the train dataset: {df_train.shape}\")\n","      print(f\"Shape of the test dataset: {df_test.shape}\")\n","\n","      return df_train, df_test\n","    except Exception as ex:\n","      print(f'Exception: {ex}')\n","      print(traceback.print_exc())\n","      return None, None\n","    finally:\n","      print('-'*50)\n","\n","  def DatasetCleaning(self,df_data):\n","    try:\n","      print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","      df_data['Gender'] = df_data['Gender'].replace('Fe Male', 'Female')\n","\n","      df_data = df_data.drop_duplicates(subset=['CustomerID'], keep='first').reset_index(drop=True)\n","\n","      for clmn in df_data.columns:\n","        if df_data[clmn].dtype in ['int64']:\n","          #print(f\"{clmn} replacing the missing value with median\")\n","          df_data[clmn] = df_data[clmn].fillna(df_data[clmn].median())\n","        else:\n","          #print(f\"{clmn} replacing the missing value with mode\")\n","          df_data[clmn] = df_data[clmn].fillna(df_data[clmn].mode()[0])\n","\n","      df_data = df_data.drop(['CustomerID'], axis=1)\n","\n","      numerical_column = df_data.select_dtypes(include=['int64'])\n","\n","      for num_col in numerical_column:\n","        Q1 = df_data[num_col].quantile(0.25)\n","        Q3 = df_data[num_col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower = Q1 - 1.5*IQR\n","        upper = Q3 + 1.5*IQR\n","        #df_data[num_col] = df_data[num_col].clip(lower,upper)\n","\n","      return df_data\n","\n","    except Exception as ex:\n","      print(f\"Exception {ex}\")\n","      print(traceback.print_exc())\n","      return None\n","    finally:\n","      print('-'*50)\n","\n","  def UploadIntoHF(self,df,drive_path,file_name):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      file_path = os.path.join(drive_path,file_name)\n","      df.to_csv(file_path,index=False)\n","\n","      api = HfApi(token = self.hf_token)\n","      api.upload_file(path_or_fileobj =file_path,\n","                      path_in_repo= f\"Master/Data/{file_name}\",\n","                      repo_id = self.repoID,\n","                      repo_type='dataset',\n","                      token=self.hf_token)\n","      print(f\"Source data {file_name} uploaded into {self.repoID}\")\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def ToRunPipeline(self):\n","    try:\n","      print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","      df_dataset = self.LoadDatasetFromHF()\n","      if df_dataset is None:\n","        return False\n","      else:\n","        df_train, df_test = self.TrainTestSplit(df_dataset)\n","        if df_train is None or df_test is None:\n","          return False\n","        else:\n","          df_train_cleaned = self.DatasetCleaning(df_train)\n","          df_test_cleaned = self.DatasetCleaning(df_test)\n","          if df_train is None or df_test is None:\n","            return False\n","          else:\n","            result_train = self.UploadIntoHF(df_train_cleaned,\n","                                             self.Subfolders,'train.csv')\n","            result_test = self.UploadIntoHF(df_test_cleaned,\n","                                            self.subfolders,'test.csv')\n","            if not result_train or not result_test:\n","              print('Splitted dataset upload into HF Exception')\n","              return False\n","            else:\n","              print('Dataset downloaded from HF, Cleaned, Splitted into train and test dataset and uploaded back into HF dataset')\n","              return True\n","    except Exception as ex:\n","      print(f\"Exception message in ToRunPipeline: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)"],"metadata":{"id":"aFDYFew4-GjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.Model Building with Experimentation Tracking"],"metadata":{"id":"vpSiNqjnWIf_"}},{"cell_type":"code","source":["%%writefile Master/BuildingModels.py\n","\n","import os\n","import joblib\n","import inspect\n","import traceback\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from xgboost import XGBClassifier\n","from datasets import load_dataset\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.impute import SimpleImputer\n","from huggingface_hub.utils import RepositoryNotFoundError\n","from huggingface_hub import HfApi, create_repo, login\n","from huggingface_hub import hf_hub_download\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import precision_recall_curve, precision_score\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.metrics import recall_score, f1_score, classification_report\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","\n","class BuildingModels:\n","  def __init__(self,base_path, hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.models = {}\n","    self.best_model = None\n","    self.best_score = 0\n","    self.best_f1_score =0.0\n","    self.best_model_threshold = 0.0\n","    self.best_model_name=None\n","    self.df_train = pd.DataFrame()\n","    self.df_test = pd.DataFrame()\n","    self.feature_train = pd.DataFrame()\n","    self.feature_test = pd.DataFrame()\n","    self.target_train = pd.Series()\n","    self.target_test = pd.Series()\n","    self.base_path = base_path\n","    self.Subfolders = os.path.join(base_path,'data')\n","    self.repo_id = 'jpkarthikeyan/Tourism_Prediction_Model'\n","    self.ds_repo_id = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.repo_type = 'model'\n","    self.hf_token = hf_token\n","    self.categorical_columns = ['TypeofContact','Occupation','Gender','ProductPitched','MaritalStatus','Designation']\n","    self.numerical_columns = ['Age','CityTier','DurationOfPitch','NumberOfPersonVisiting',\n","                              'NumberOfFollowups','PreferredPropertyStar',\n","                              'NumberOfTrips','Passport','PitchSatisfactionScore','OwnCar',\n","                              'NumberOfChildrenVisiting','MonthlyIncome']\n","\n","    self.pipeline_numerical = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='median')),\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    self.pipeline_onehot = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore',sparse_output=False))\n","    ])\n","\n","  def Load_data_from_HF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      print(f'Loading the train dataset from {self.ds_repo_id}')\n","\n","      self.df_train = pd.read_csv(hf_hub_download(\n","                                repo_id = self.ds_repo_id,\n","                                filename = 'Master/Data/train.csv',repo_type='dataset'))\n","      self.df_test = pd.read_csv(hf_hub_download(\n","                                repo_id = self.ds_repo_id,\n","                                filename = 'Master/Data/test.csv',repo_type='dataset'))\n","      print(f\"Shape of the train dataset: {self.df_train.shape}\")\n","      print(f\"Shape of the train dataset: {self.df_test.shape}\")\n","\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def Preprocessing_dataset(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","\n","      self.target_train = self.df_train['ProdTaken']\n","      self.feature_train = self.df_train.drop(['ProdTaken'],axis=1)\n","\n","      self.target_test = self.df_test['ProdTaken']\n","      self.feature_test = self.df_test.drop(['ProdTaken'],axis=1)\n","\n","      return True\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def Building_Models(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      preprocessor = ColumnTransformer(\n","          transformers=[\n","              ('num', self.pipeline_numerical,self.numerical_columns),\n","              ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore',\n","                        sparse_output=False),self.categorical_columns)])\n","      models_params = {\n","          'DecisionTreeClassifier':{\n","              'model': DecisionTreeClassifier(class_weight='balanced',random_state=42),\n","              'params': {'classifier__criterion':['gini','entropy'],\n","                         'classifier__splitter':['best','random'],\n","                        'classifier__max_depth':[1],\n","                         'classifier__min_samples_leaf':[1,2,4],\n","                         'classifier__min_samples_split':[2,5,10],\n","                         'classifier__max_features':['sqrt','log2',None]}\n","          },\n","\n","          'RandomForestClassifier':{\n","              'model': RandomForestClassifier(class_weight='balanced',random_state=42),\n","              'params': { 'classifier__n_estimators':[25,50,75,100],\n","                          'classifier__criterion':['gini','entropy'],\n","                          'classifier__max_depth':[5,10,15],\n","                          'classifier__min_samples_split':[15,20,25],\n","                          'classifier__min_samples_leaf':[7,10,15],\n","                          'classifier__max_features':[0.3,0.5,0.6],\n","                          'classifier__oob_score':[True],\n","                          'classifier__bootstrap':[True]\n","                         }\n","          },\n","\n","          'BaggingClassifier':{\n","              'model': BaggingClassifier(estimator=DecisionTreeClassifier(class_weight='balanced',random_state=42)),\n","              'params':{  'classifier__n_estimators':[10,50,75,100],\n","                          'classifier__max_samples':[0.3,0.5,0.7,0.9],\n","                          'classifier__max_features':[0.3,0.5,0.7],\n","                          'classifier__oob_score':[True],\n","                          'classifier__estimator__criterion':['gini','entropy'],\n","                          'classifier__estimator__max_depth':[5,7,9],\n","                          'classifier__estimator__min_samples_split':[8,10,12],\n","                          'classifier__estimator__min_samples_leaf':[2,3,5]\n","                        }\n","          },\n","\n","          'AdaBoostingClassifier':{\n","              'model': AdaBoostClassifier(random_state=42),\n","              'params':{  'classifier__n_estimators':[50,75,100],\n","                          'classifier__learning_rate':[0.01,0.05,0.1],\n","                          'classifier__algorithm':['SAMME','SAMME.R']\n","\n","                      }\n","          },\n","\n","          'GradientBoostingClassifier':{\n","              'model': GradientBoostingClassifier(random_state=42),\n","              'params':{\n","                          'classifier__n_estimators':[50,75,100,125],\n","                          'classifier__learning_rate':[0.01,0.5,0.1],\n","                          'classifier__criterion':['friedman_mse','squared_error'],\n","                          'classifier__max_features':['sqrt','log2'],\n","                          'classifier__min_samples_leaf':[1,2,4],\n","                          'classifier__subsample':[0.6,0.7,0.8],\n","                          'classifier__max_depth':[2,3,4,5]\n","                        }\n","          },\n","\n","          'XGBoostingClassifier':{\n","              'model':XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n","              'params':{'classifier__n_estimators':np.arange(50,100,10),\n","                        'classifier__max_depth': [3,5,7],\n","                        'classifier__learning_rate':[0.01,0.1,0.2],\n","                        'classifier__subsample':[0.6,0.8,1.0],\n","                        'classifier__colsample_bytree':[0.6,0.8,1.0],\n","                        'classifier__gamma':[0,1,2],\n","                        'classifier__reg_alpha':[0,1,2]\n","\n","                        }\n","          }\n","\n","        }\n","\n","      cv_KFold = KFold(n_splits=3,random_state=42,shuffle=True)\n","\n","      for model_name, mdl_params in models_params.items():\n","        print(f'Model {model_name} started')\n","        pipeline = Pipeline(steps=[\n","            ('preprocessor',preprocessor),\n","            ('classifier',mdl_params['model'])\n","        ])\n","        random_search = RandomizedSearchCV(pipeline,mdl_params['params'],\n","                                           n_iter=50,cv=cv_KFold,scoring='f1',\n","                                           random_state=42,n_jobs=-1,verbose=2)\n","\n","        random_search.fit(self.feature_train,self.target_train)\n","\n","        self.models[model_name] = {\n","            'model':random_search.best_estimator_,\n","            'best_score': random_search.best_score_,\n","            'best_params':random_search.best_params_\n","        }\n","        joblib.dump(random_search.best_estimator_,f'{self.base_path}/Model_Dump_JOBLIB/{model_name}.joblib')\n","        print(f'model:{random_search.best_estimator_}')\n","        print(f'best_score: {random_search.best_score_}')\n","        print(f'best_params: {random_search.best_params_}')\n","        print(f'Modle {model_name} completed')\n","        print('-'*50)\n","\n","      return self.models\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","    finally:\n","      print('-'*50)\n","\n","  def Model_Evaluation(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    df_metrics = pd.DataFrame()\n","    try:\n","      for mdl_name, mdl_info in self.models.items():\n","        model = mdl_info['model']\n","        predict_proability = model.predict_proba(self.feature_test)\n","        print(f\"Predict proability shape {mdl_name} {predict_proability.shape}\")\n","        if predict_proability.shape[1] ==1:\n","          predict_proability = predict_proability.flatten()\n","        else:\n","          predict_proability = predict_proability[:,1]\n","\n","\n","        prc_precision,prc_recall, prc_threshold = precision_recall_curve(self.target_test,predict_proability)\n","        prc_f1score = 2*((prc_precision*prc_recall) / (prc_precision+prc_recall+1e-10))\n","\n","        prc_threshold_idmx = np.argmax(prc_f1score)\n","        prc_best_threshold = prc_threshold[prc_threshold_idmx]\n","        print(f'best threshold: {prc_best_threshold}')\n","\n","        predic_prob_threshold = (predict_proability >= prc_best_threshold).astype(int)\n","        #predic_prob_threshold = (predict_proability >= 0.5).astype(int)\n","        accuracy = accuracy_score(self.target_test,predic_prob_threshold)\n","        precision = precision_score(self.target_test,predic_prob_threshold)\n","        recall = recall_score(self.target_test,predic_prob_threshold)\n","        f1score = f1_score(self.target_test,predic_prob_threshold)\n","        class_report = classification_report(self.target_test,predic_prob_threshold)\n","        conf_matrix = confusion_matrix(self.target_test,predic_prob_threshold)\n","\n","        lbl = ['TN', 'FP', 'FN', 'TP']\n","        cnf_lbl = ['\\n{0:0.0f}'.format(cnf_val) for cnf_val in conf_matrix.flatten()]\n","        cn_percentage = [\"\\n{0:.2%}\".format(item/conf_matrix.flatten().sum()) for item in conf_matrix.flatten()]\n","\n","        confusion_label = np.asarray([[\"\\n {0:0.0f}\".format(item)+\"\\n{0:.2%}\".format(item/conf_matrix.flatten().sum())]\n","                                for item in conf_matrix.flatten()]).reshape(2,2)\n","\n","        cnf_label = np.asarray([f'{lbl1} {lbl2} {lbl3}' for lbl1, lbl2, lbl3 in zip(lbl, cnf_lbl,  cn_percentage)]).reshape(2,2)\n","\n","        plt.figure(figsize = (3,3))\n","        sns.heatmap(conf_matrix, annot = cnf_label, cmap = 'Spectral', fmt='' )\n","        plt.xlabel('Predicted')\n","        plt.ylabel('Actual')\n","        plt.title(f'{mdl_name} confusion matrix')\n","        plt.tight_layout()\n","        plt.show()\n","\n","        df_metrics = pd.concat([df_metrics,pd.DataFrame({'model':[mdl_name],'accuracy':[accuracy],\n","                                            'precision':[precision], 'recall':[recall],\n","                                            'f1_score':[f1score]})],ignore_index=True)\n","\n","        if f1score > self.best_f1_score:\n","          self.best_f1_score = f1score\n","          self.best_model_threshold = prc_best_threshold\n","          self.best_model_name = mdl_name\n","\n","      best_model = self.models[self.best_model_name]['model']\n","      if hasattr(best_model, 'feature_importances_'):\n","        feature_importance = pd.DataFrame({\n","            'feature':self.feature_train.columns,\n","            'importance': best_model.feature_importances_\n","        }).sort_values('importance',ascending=False)\n","        print('Feature Importance:\\n',feature_importance)\n","\n","\n","      return df_metrics\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","    finally:\n","      print('-'*50)\n","\n","  def Register_BestModel_HF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      best_model = self.models[self.best_model_name]['model']\n","      joblib.dump(best_model,f'{self.base_path}/Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib')\n","\n","\n","      api = HfApi()\n","      try:\n","        api.repo_info(repo_id=self.repo_id,repo_type=self.repo_type)\n","      except RepositoryNotFoundError:\n","        api.create_repo(repo_id=self.repo_id, repo_type=self.repo_type,private=False)\n","\n","\n","      print(\"Uploading the best model into Hugging face\")\n","      api.upload_file(path_or_fileobj = f'{self.base_path}/Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib',\n","                      path_in_repo = f\"Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib\",\n","                      repo_id=self.repo_id, repo_type=self.repo_type\n","                      )\n","\n","\n","      print(\"Uploading the best threshold text file to HF\")\n","      with open('Master/Model_Dump_JOBLIB/best_threshold.txt','w') as f:\n","        f.write(str(self.best_model_threshold))\n","      api.upload_file(path_or_fileobj = f\"{self.base_path}/Model_Dump_JOBLIB/best_threshold.txt\",\n","                      path_in_repo = f\"Model_Dump_JOBLIB/best_threshold.txt\",\n","                      repo_id=self.repo_id, repo_type=self.repo_type\n","                      )\n","\n","      return True\n","\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def ToRunPipeline(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      if not self.Load_data_from_HF():\n","        return False\n","      else:\n","        if not self.Preprocessing_dataset():\n","          return False\n","        else:\n","          Build_Model = self.Building_Models()\n","          print(Build_Model)\n","          if Build_Model:\n","            df_Metrics = self.Model_Evaluation()\n","            print(df_Metrics)\n","            if not df_Metrics.empty:\n","              if self.Register_BestModel_HF():\n","                return True\n","              else:\n","                return False\n","            else:\n","              return False\n","          else:\n","            return False\n","    except Exception as ex:\n","      print(f'Exception occured {ex}')\n","    finally:\n","      print('-'*50)\n"],"metadata":{"id":"Og90ls1JWWto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Function"],"metadata":{"id":"VmVIs57DX9vR"}},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1/Master'"],"metadata":{"id":"DvSDjubyxhGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile main.py\n","import os\n","import sys\n","import argparse\n","import logging\n","import traceback\n","from huggingface_hub import HfApi\n","from dotenv import load_dotenv\n","\n","\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='pipeline.log')\n","base_path = os.path.abspath(os.path.join(os.path.dirname(__file__),'..','Master'))\n","hf_token = os.getenv('HF_TOKEN')\n","\n","logging.info(f\"Base Path {base_path}\")\n","sys.path.append(base_path)\n","\n","if not hf_token:\n","  logging.error(\"HF_TOKEN not found\")\n","  sys.exit(1)\n","\n","api = HfApi(token = hf_token)\n","try:\n","  user = api.whoami()\n","  logging.error(f\"Authenticated as :{user['name']}\")\n","except Exception as ex:\n","  logging.error(f\"TokenError: {ex}\")\n","  sys.exit(1)\n","\n","\n","parser = argparse.ArgumentParser(description = 'To Run a specific job in a pipileine')\n","parser.add_argument('--job', type=str, required=True, choices=['register','prepare','modelbuilding','deploy'], help ='Job to execute: register, prepare, modelbuilding and deploy in hugging face')\n","\n","args = parser.parse_args()\n","\n","data_dir = os.path.join(base_path,'Data')\n","logging.info(f\"data_dir: {data_dir}\")\n","if os.path.exists(data_dir):\n","  os.makedirs(data_dir, exist_ok=True)\n","\n","model_dir = os.path.join(base_path,'Model_Dump_JOBLIB')\n","logging.info(f\"model_dir: {model_dir}\")\n","if os.path.exists(model_dir):\n","  os.makedirs(model_dir, exist_ok=True)\n","\n","if args.job == 'register':\n","  from DataRegistration import DataRegistration\n","  ObjDataReg = DataRegistration(base_path,hf_token)\n","  if not ObjDataReg.ToRunPipeline():\n","    logging.error(\"DataRegistration failed\")\n","    sys.exit(1)\n","  else:\n","    logging.info(\"DataRegistration completed in Hugging Face\")\n","\n","if args.job == 'prepare':\n","  from DataPrepration import DataPrepration\n","  ObjDataprep = DataPrepration(base_path,hf_token)\n","  if not ObjDataprep.ToRunPipeline():\n","    logging.error(\"Data Registration failed\")\n","    sys.exit(1)\n","  else:\n","    logging.info(\"Data prepration completed\")\n","\n","if args.job == 'modelbuilding':\n","  from ModelBuilding import BuildingModels\n","  ObjModelBuild = BuildingModels(base_path, hf_token)\n","  if not ObjModelBuild.ToRunPipeline():\n","    logging.error(\"Model Building failed\")\n","    sys.exit(1)\n","  else:\n","    logging.info(\"Model Building Completed\")\n","\n","\n","\n","\n","\n"],"metadata":{"id":"9Sx49q1ym8FF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755277215072,"user_tz":-330,"elapsed":35,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"8017e935-17f7-41c9-ea7e-f87b7d4e812f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}]},{"cell_type":"markdown","source":["#Front End Implementation"],"metadata":{"id":"-PWTuTZHusYj"}},{"cell_type":"code","source":["pip install streamlit"],"metadata":{"id":"IppdmKSyu3G-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/requirements.txt\n","pandas\n","numpy\n","scikit-learn\n","joblib\n","streamlit\n","huggingface_hub\n","xgboost"],"metadata":{"id":"dW90Nwy5rQE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/README.md\n","---\n","title: Visit With Us - Tourism package prediction\n","emoji: 🚩\n","colorFrom: blue\n","colorTo: green\n","sdk: docker\n","sdk_version: 3.9\n","app_file: app.py\n","app_type: streamlit\n","pinned: false\n","license: mit\n","---\n","The streamlit app predicts the customer will purchace the tourism package"],"metadata":{"id":"mrU_4ijQrl_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/Dockerfile\n","# Use a minimal base image with Python 3.9 installed\n","FROM python:3.9-slim\n","\n","# Set the working directory inside the container to /app\n","WORKDIR /app\n","\n","# Copy all files from the current directory on the host to the container's /app directory\n","COPY . .\n","\n","# Install Python dependencies listed in requirements.txt\n","RUN pip install --no-cache-dir -r requirements.txt\n","RUN mkdir -p /tmp/hf_cache && chmod -R 777 /tmp/hf_cache\n","ENV HF_HOME=/tmp/hf_cache\n","ENV HUGGINGFACE_HUB_CACHE=/tmp/hf_cache\n","ENV PYTHONUNBUFFERED=1\n","\n","\n","EXPOSE 7860\n","\n","\n","# Define the command to run the Streamlit app on port \"7860\" and make it accessible externally\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=7860\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"],"metadata":{"id":"F96pJ8MK1jPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/app.py\n","import streamlit as st\n","import pandas as pd\n","import joblib\n","import os\n","import logging\n","from huggingface_hub import login,hf_hub_download\n","from xgboost import XGBClassifier\n","#from google.colab import userdata\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","cache_dir = \"/tmp/hf_cache\"\n","os.environ[\"HF_HOME\"] = cache_dir\n","os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_dir\n","\n","try:\n","  hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n","\n","  if hf_token:\n","    login(token=hf_token)\n","    logger.info(\"Successfully logged in to Hugging Face\")\n","  else:\n","    logger.error(\"Hugging face token not found\")\n","    st.error(\"Huggingface token not found\")\n","except Exception as ex:\n","  logger.error(f\"Failed to login to Hugging face: {ex} \")\n","  st.write(f\"Failed to login to Hugging face: {ex} \")\n","\n","try:\n","  os.makedirs(cache_dir, exist_ok=True)\n","  logger.info(f\"Created cache directory {cache_dir}\")\n","except Exception as ex:\n","  logger.error(f\"Failed to create cache directory {cache_dir}: {ex}\")\n","  st.error(f\"Failed to create cache directory {cache_dir}: {ex}\")\n","\n","\n","st.title(\"Visit with Us: Tourism Package Prediction\")\n","st.write(\"Enter the Customer details to predict the likehood of purchasing the tourism packages\")\n","\n","\n","if 'predictor' not in st.session_state:\n","  st.session_state.predictor = None\n","  st.session_state.model_loaded = False\n","\n","class PredictorTourism:\n","\n","  def __init__(self):\n","    self.Subfolders = 'Master'\n","    self.repoID = 'jpkarthikeyan/Tourism_Prediction_Model'\n","    self.model = None\n","    self.best_threshold = 0.0\n","\n","  def Load_Model(self):\n","    try:\n","      logger.info(\"Loading best model\")\n","      model_path = hf_hub_download(\n","          repo_id = self.repoID,filename = f'Model_Dump_JOBLIB/BestModel_XGBoostingClassifier.joblib',\n","          repo_type = 'model')\n","      threshold_path = hf_hub_download(\n","          repo_id = self.repoID, filename=f'Model_Dump_JOBLIB/best_threshold.txt',\n","          repo_type='model')\n","\n","      logger.info(f\"Model path: {model_path}\")\n","      logger.info(f\"Threshold path:  {threshold_path}\")\n","\n","      self.model = joblib.load(model_path)\n","      with open(threshold_path,'r') as f:\n","        self.best_threshold = float(f.read())\n","      st.success(\"Model and threshold loaded successfully\")\n","      return True\n","\n","    except Exception as ex:\n","      st.error(f'Exception: {ex}')\n","      return False\n","\n","\n","  def Predict(self, data):\n","    try:\n","      logger.info(f\"Input Data: {data}\")\n","      df= pd.DataFrame([data])\n","      logger.info(f\"Data shape: {df.shape}\")\n","      logger.info(f\"Dataframe columns: {df.columns.tolist()}\")\n","      prob = self.model.predict_proba(df)[:,1]\n","      prediction = int(prob >= self.best_threshold)\n","      return prediction\n","\n","    except Exception as ex:\n","      logger.error(f\"Exception in predict: {ex}\", exc_info=True)\n","      st.error(f\"Exception Prediction: {ex} {traceback.print_exc}\")\n","      return ex\n","\n","\n","if not st.session_state.model_loaded:\n","  st.session_state.predictor = PredictorTourism()\n","  st.session_state.model_loaded = st.session_state.predictor.Load_Model()\n","\n","with st.form(\"customer_form\"):\n","  st.header(\"Customer Details\")\n","  col1, col2,col3 = st.columns(3)\n","\n","  with col1:\n","\n","    age = st.number_input(\"Age\", min_value=18, max_value=100, value=41)\n","    gender = st.selectbox('Gender',['Male','Female'])\n","    MaritalStatus = st.selectbox('MaritalStatus',['Married','Unmarried','Single','Divorced'])\n","    Occupation = st.selectbox('Occupation',['Free Lancer','Salaried','Small Business','Large Business'])\n","    Designation = st.selectbox('Designation',['AVP','Manager','Executive','Senior Manager','VP'])\n","    MonthlyIncome = st.number_input('MonthlyIncome',min_value=0, max_value=1000000,value=20999)\n","\n","  with col2:\n","\n","    typeofcontact = st.selectbox(\"TypeofContact\",['Self Enquiry','Company Invited'])\n","    citytier = st.selectbox('citytier',[1,2,3], index=2)\n","    DurationOfPitch = st.number_input('DurationOfPitch', min_value=1, max_value=60, value=6)\n","    ProductPitched = st.selectbox('ProductPitched',['Deluxe','Basic','Standard','Super Deluxe','King'])\n","    PreferredPropertyStar = st.selectbox(\"'PreferredPropertyStar\",[3,2,1])\n","    NumberOfTrips = st.number_input('NumberOfTrips',min_value=0, max_value=30, value=1)\n","\n","\n","  with col3:\n","    NumberOfPersonVisiting = st.number_input('NumberOfPersonVisiting',min_value=1,max_value=10,value=3)\n","    NumberOfFollowups = st.number_input('NumberOfFollowups',min_value=0,max_value=10, value=3)\n","    NumberOfChildrenVisiting= st.number_input('NumberOfChildrenVisiting',min_value=0,max_value=5,value=0)\n","    Passport= st.selectbox('Passport',['Yes','No'],format_func=lambda x:\"Yes\" if x==\"Yes\" else \"No\")\n","    Owncar= st.selectbox('OwnCar',['Yes','No'],format_func=lambda x:\"Yes\" if x==\"Yes\" else \"No\")\n","    PitchSatisfactionScore= st.number_input('PitchSatisfactionScore',min_value=1,max_value=5,value=3)\n","\n","\n","  submitted = st.form_submit_button(\"Predict\")\n","\n","if submitted:\n","  input_data = {\n","      'Age':age,\n","      'TypeofContact':typeofcontact,\n","      'CityTier':citytier,\n","      'DurationOfPitch':DurationOfPitch,\n","      'Occupation':Occupation,\n","      'Gender':gender,\n","      'NumberOfPersonVisiting':NumberOfPersonVisiting,\n","      'NumberOfFollowups':NumberOfFollowups,\n","      'ProductPitched':ProductPitched,\n","      'PreferredPropertyStar':PreferredPropertyStar,\n","      'MaritalStatus':MaritalStatus,\n","      'NumberOfTrips':NumberOfTrips,\n","      'Passport':1 if Passport ==\"Yes\" else 0,\n","      'OwnCar':1 if Owncar ==\"Yes\" else 0,\n","      'PitchSatisfactionScore':PitchSatisfactionScore,\n","      'NumberOfChildrenVisiting':NumberOfChildrenVisiting,\n","      'Designation':Designation,\n","      'MonthlyIncome':MonthlyIncome\n","\n","  }\n","\n","\n","  if st.session_state.predictor:\n","    result = st.session_state.predictor.Predict(input_data)\n","\n","    if result is not None:\n","      st.subheader(f\"Prediction Result is {result}\")\n","      st.write(f\"Likely to purchase\" if result ==1 else \"Unlikely to purchase\")\n","    else:\n","      st.write(result)\n","      st.error(\"Error in prediction\")\n","  else:\n","    st.error(\"Models are not loaded, please ensure the model and threshold are available on Hugging face\")\n","\n"],"metadata":{"id":"MgAoOlhJvV_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"0fy4-QqVKvYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile HostingInHuggingFace.py\n","import os\n","import inspect\n","import traceback\n","from huggingface_hub import HfApi, create_repo, login,hf_hub_download\n","from huggingface_hub.utils import RepositoryNotFoundError\n","\n","class HostingInHuggingFace:\n","  def __init__(self,base_path,hf_token=None):\n","    self.base_path = base_path\n","    self.hf_token = hf_token\n","    self.repo_id = 'jpkarthikeyan/Tourism-Prediction-Model-Space'\n","\n","  def CreatingSpaceInHF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    api = HfApi()\n","    try:\n","      print(f\"Checking for {self.repo_id} is correct or not\")\n","      api.repo_info(repo_id = self.repo_id,\n","                    repo_type='space',\n","                    token = self.hf_token)\n","      print(f\"Space {self.repo_id} already exists\")\n","    except RepositoryNotFoundError:\n","      create_repo(repo_id=self.repo_id,repo_type='space',\n","                  space_sdk='docker',private=False)\n","      print(f\"Space created in {self.repo_id}\")\n","\n","\n","  def UploadDeploymentFile(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      api = HfApi(token=self.hf_token)\n","      directory_to_upload = os.path.join(self.base_path,'Deployment')\n","      print(f\"Directory to upload {directory_to_upload} into HF Space {self.repo_id}\")\n","      api.upload_folder(repo_id=self.repo_id,folder_path=directory_to_upload,repo_type='space')\n","      print(f\"Successfully upload {directory_to_upload} into {self.repo_id}\")\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception occured {ex}\")\n","      print(traceback.print_exc())\n","      raise\n","    finally:\n","      print('-'*50)\n","\n","  def ToRunPipeline(self):\n","    try:\n","      self.CreatingSpaceInHF()\n","      self.UploadDeploymentFile()\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception occured {ex}\")\n","      print(traceback.print_exc())\n","      raise\n","    finally:\n","      print('-'*50)\n"],"metadata":{"id":"x_puqsQWuv-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# pipeline.yml"],"metadata":{"id":"2XBCf10RzakL"}},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC13l7HATg8x","executionInfo":{"status":"ok","timestamp":1755274156371,"user_tz":-330,"elapsed":27,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"1e915d41-22e5-47ab-ef62-43117c91fdfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvpBtUiUTiuF","executionInfo":{"status":"ok","timestamp":1755274157990,"user_tz":-330,"elapsed":169,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"681ece8b-e5b9-4550-d0b8-e88c375b73f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Master\n"]}]},{"cell_type":"code","source":["%%writefile .github/workflows/pipeline.yml\n","name: VISIT WITH US TOURISM PREDICTION PIPELINE\n","on:\n","  push:\n","    branches:\n","      - main\n","    paths:\n","      - 'Master/Deployment/**'\n","      - 'Master/Data/tourism.csv'\n","      - 'Master/main.py'\n","      - 'Master/DataRegistration.py'\n","      - 'Master/DataPrepration.py'\n","      - 'Master/ModelBuilding.py'\n","      - 'Master/HostingInHuggingFace.py'\n","      - '.github/workflows/pipeline.yml'\n","  workflow_dispatch:\n","\n","jobs:\n","  data_registration:\n","    runs-on: ubuntu-latest\n","    steps:\n","      - name: Checkout Repository\n","        uses: actions/checkout@v4\n","\n","      - name: Set up Python\n","        uses: actions/setup-python@v4\n","        with:\n","          python-version: '3.12'\n","\n","      - name: Install Dependencies\n","        run: |\n","          python -m pip install --upgrade pip\n","          pip install huggingface_hub python-dotenv pandas\n","\n","      - name: List Direcory contents(debug)\n","        run: |\n","          ls -la Master/Data/ || echo \"Master/Data/ Directory not found\"\n","          ls -la . || echo \"Root Directory Contents\"\n","\n","      - name: Copy Dataset File\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: |\n","          mkdir -p Master/Data\n","          if [ -f Master/Data/tourism.csv ]; then\n","            echo \"tourism.csv found in Master/Data\"\n","          else\n","            echo \"tourism.csv not found in Master/Data/\"\n","          fi\n","\n","      - name: Run DataRegistration.py\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN}}\n","        run: |\n","          cd Master/\n","          python main.py --job register\n","\n","      - name: Check Pipeline Status\n","        if: failure()\n","        run: |\n","          echo \"DataRegistration pipeline failed. please check logs\"\n","          exit 1\n","\n","      - name: Verify Upload\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: |\n","          echo \"Verifying Upload on Hugging Face\"\n","          python -c \"import os; from huggingface_hub import HfApi; api=HfApi(token=os.getenv('HF_TOKEN'));print(api.repo_info(repo_id='jpkarthikeyan/Tourism-Visit-With-us-dataset',repo_type='dataset))\""],"metadata":{"id":"WjL8ua--5jUx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755276854677,"user_tz":-330,"elapsed":36,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"cc98990c-95f2-4e5c-eeb7-b8a6d9b27189"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting .github/workflows/pipeline.yml\n"]}]}]}