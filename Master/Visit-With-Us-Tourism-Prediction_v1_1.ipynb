{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["K7HT7f_W63Q9","gaE_eACI7Wdl","UQ_4lbVH0fTI","2XxU9ZdG-DE9","vpSiNqjnWIf_","-PWTuTZHusYj","fpyLr8jCEKkm"],"mount_file_id":"1YSQsHoWmBi5rFxowpmN5eflI65FI1c7n","authorship_tag":"ABX9TyMbvzCTQMNL7kR4eOeFqZ9X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Project Folder Structure"],"metadata":{"id":"K7HT7f_W63Q9"}},{"cell_type":"markdown","source":["Project Folder Structure\n","\n","|->`Drive`/VisitWithUs-Tourism\n","\n","  |->Master\n","  \n","    |->Data\n","        |->tourism.csv\n","        |->train.csv\n","        |->test.csv\n","    |->Deployment\n","        |->app.py\n","        |->Dockerfile\n","        |->README.md\n","        |->requirements.txt\n","    |->Model_Building\n","        |->BuildingModels.py\n","        |->DataPrepration.py\n","        |->DataRegistration.py\n","    |->Model_Dump_JOBLIB\n","        |->best_threshold.txt\n","        |->BestModel_XGBoostingClassifier.joblib\n","    |->pipeline.yml\n","  |->AIML_MLOPS_v1_1.ipynb"],"metadata":{"id":"FMxBY8Nqd0ve"}},{"cell_type":"markdown","source":["# Loading Packages"],"metadata":{"id":"gaE_eACI7Wdl"}},{"cell_type":"code","source":["!pip install huggingface_hub\n","!pip install python-dotenv\n","!pip install datasets\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install xgboost\n","!pip install seaborn\n","!pip install matplotlib\n","!pip install joblib\n","!pip install stramlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zzL9Lcv7HHX","executionInfo":{"status":"ok","timestamp":1755229130166,"user_tz":-330,"elapsed":67277,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"d663b224-2b40-4b75-8e20-ffe378f76032"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.8.3)\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.1.1\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.23.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement stramlit (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for stramlit\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["# MOUNTING DRIVE"],"metadata":{"id":"ognKAWMS8G_j"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/'\n","# base_path = 'VisitWithUs-Tourism_version_1_1/Master/'\n","# print(f\"Base Path {base_path}\")\n","\n","# data_path = os.path.join(base_path, 'Data')\n","# model_joblib_path = os.path.join(base_path, 'Model_Dump_JOBLIB')\n","\n","# print(\"checking for Program folder created or not\")\n","# if not os.path.exists(base_path):\n","#   os.makedirs(base_path, exist_ok=True)\n","# else:\n","#   print(f\"folder already exists {base_path}\")\n","\n","# if not os.path.exists(data_path):\n","#   os.makedirs(data_path, exist_ok=True)\n","# print(f\"folder already exists {data_path}\")\n","\n","# if not os.path.exists(model_joblib_path):\n","#   os.makedirs(model_joblib_path, exist_ok=True)\n","# print(f\"folder already exists {model_joblib_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vw9tngctf_m5","executionInfo":{"status":"ok","timestamp":1755246749450,"user_tz":-330,"elapsed":1793,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"4c1ad01f-3af8-4b34-82de-e644fc9c7fdb"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-jjy_z2g2Zq","executionInfo":{"status":"ok","timestamp":1755229176927,"user_tz":-330,"elapsed":109,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"4bb24464-6506-4800-e8f6-7f2fb9facc46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Reference  'version 0'  'version 0.1'\t VisitWithUs-Tourism_version_1_1\n"]}]},{"cell_type":"markdown","source":["# 1. DATA REGISTRATION"],"metadata":{"id":"UQ_4lbVH0fTI"}},{"cell_type":"code","source":["# @title DataRegistration Class\n","#%%writefile $base_path/DataRegistration.py\n","import os\n","import traceback\n","import inspect\n","from huggingface_hub import HfApi, create_repo,login,hf_hub_download\n","\n","class DataRegistration:\n","  def __init__(self,base_path,hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.repoID = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.Subfolders = os.path.join(base_path,'Data')\n","    self.folder_Master = base_path\n","    self.folder_data = os.path.join(base_path,\"Data\")\n","    self.hf_token = hf_token\n","    os.makedirs(self.folder_data, exist_ok=True)\n","    print(f\"self.Subfolders: {self.Subfolders}\")\n","    print(f\"self.folder_Master: {self.folder_Master}\")\n","    print(f\"folder_data: {self.folder_data}\")\n","\n","  def HFCreateRepo(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      create_repo(repo_id=self.repoID,\n","                  private=False,\n","                  repo_type='dataset',\n","                  exist_ok=True)\n","      print(f\"Repo {self.repoID} created\")\n","      return True\n","\n","    except Exception as ex:\n","      if hasattr(ex,'response') and ex.response.status_code == 409:\n","        print(f\"Repo {self.repoID} already exists\")\n","        return True\n","      else:\n","        print(f\"Exception {ex}\")\n","        traceback.print_exc()\n","        return False\n","    finally:\n","      print(\"-\"*100)\n","\n","\n","  def UploadingSourceData(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      source_data_file = os.path.join(self.folder_data,'tourism.csv')\n","      print(f\"Soruce Data File {source_data_file}\")\n","      if not os.path.exists(source_data_file):\n","        raise FileNotFoundError(f\"File {source_data_file} not found\")\n","      api = HfApi()\n","      api.upload_file(\n","          path_or_fileobj = source_data_file,\n","          path_in_repo = 'Master/Data/tourism.csv',\n","          repo_id = self.repoID,\n","          repo_type='dataset',\n","          token=self.hf_token)\n","      print(f\"Source data tourism.csv uploaded into {self.repoID}\")\n","      return True\n","\n","    except Exception as ex:\n","       print(f\"Exception at {inspect.currentframe().f_code.co_name} Exception: {ex}\")\n","       traceback.print_exc()\n","       return False\n","    finally:\n","      print(\"-\"*100)\n","\n","  def ToRunPipeline(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    if not self.HFCreateRepo():\n","      print('Exception in data registration HFCreateRepo')\n","      return False\n","    else:\n","      if not self.UploadingSourceData():\n","        print('Exception in data registration UploadingSourceData')\n","        return False\n","      else:\n","        print('Data Registration Completed')\n","        return True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDUqHrVl0q7o","executionInfo":{"status":"ok","timestamp":1755229184164,"user_tz":-330,"elapsed":754,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"d8c8e94f-9b39-4497-c551-d2cf83aafa7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting VisitWithUs-Tourism_version_1_1/Master//DataRegistration.py\n"]}]},{"cell_type":"markdown","source":["# 2. Data Prepration"],"metadata":{"id":"2XxU9ZdG-DE9"}},{"cell_type":"code","source":["#%%writefile $base_path/DataPrepration.py\n","import os\n","import pandas as pd\n","import inspect\n","import traceback\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","from huggingface_hub import HfApi, create_repo, login, hf_hub_download\n","\n","class DataPrepration:\n","  def __init__(self,base_path, hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.repoID = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.Subfolders = os.path.join(base_path, 'Data')\n","    self.hf_token = hf_token\n","\n","  def LoadDatasetFromHF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      df_dataset = pd.read_csv(hf_hub_download(\n","                                repo_id = self.repoID,\n","                                filename = 'Master/Data/tourism.csv',\n","                                repo_type='dataset'\n","                              ))\n","\n","      print(f'Shape of the original dataset {df_dataset.shape}')\n","\n","      if 'Unnamed: 0' in df_dataset.columns:\n","        df_dataset = df_dataset.drop(['Unnamed: 0'],axis=1)\n","\n","      print(f\"Dataset loaded from {self.repoID}/{self.Subfolders}\")\n","      print(f\"Shape of the Original Dataset: {df_dataset.shape}\")\n","      return df_dataset\n","    except Exception as ex:\n","      print(f\"Exception {ex}\")\n","      traceback.print_exc()\n","      return None\n","    finally:\n","      print('-'*50)\n","\n","  def TrainTestSplit(self,df_dataset):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      print(f\"Value Count {df_dataset['ProdTaken'].value_counts()}\")\n","\n","      df_train,df_test = train_test_split(df_dataset,\n","                                          test_size=0.2,\n","                                          random_state=42,\n","                                          stratify=df_dataset['ProdTaken'],\n","                                          shuffle=True)\n","\n","      print(f\"Shape of the train dataset: {df_train.shape}\")\n","      print(f\"Shape of the test dataset: {df_test.shape}\")\n","\n","      return df_train, df_test\n","    except Exception as ex:\n","      print(f'Exception: {ex}')\n","      print(traceback.print_exc())\n","      return None, None\n","    finally:\n","      print('-'*50)\n","\n","  def DatasetCleaning(self,df_data):\n","    try:\n","      print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","      df_data['Gender'] = df_data['Gender'].replace('Fe Male', 'Female')\n","\n","      df_data = df_data.drop_duplicates(subset=['CustomerID'], keep='first').reset_index(drop=True)\n","\n","      for clmn in df_data.columns:\n","        if df_data[clmn].dtype in ['int64']:\n","          #print(f\"{clmn} replacing the missing value with median\")\n","          df_data[clmn] = df_data[clmn].fillna(df_data[clmn].median())\n","        else:\n","          #print(f\"{clmn} replacing the missing value with mode\")\n","          df_data[clmn] = df_data[clmn].fillna(df_data[clmn].mode()[0])\n","\n","      df_data = df_data.drop(['CustomerID'], axis=1)\n","\n","      numerical_column = df_data.select_dtypes(include=['int64'])\n","\n","      for num_col in numerical_column:\n","        Q1 = df_data[num_col].quantile(0.25)\n","        Q3 = df_data[num_col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower = Q1 - 1.5*IQR\n","        upper = Q3 + 1.5*IQR\n","        #df_data[num_col] = df_data[num_col].clip(lower,upper)\n","\n","      return df_data\n","\n","    except Exception as ex:\n","      print(f\"Exception {ex}\")\n","      print(traceback.print_exc())\n","      return None\n","    finally:\n","      print('-'*50)\n","\n","  def UploadIntoHF(self,df,drive_path,file_name):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      file_path = os.path.join(drive_path,file_name)\n","      df.to_csv(file_path,index=False)\n","\n","      api = HfApi(token = self.hf_token)\n","      api.upload_file(path_or_fileobj =file_path,\n","                      path_in_repo= f\"Master/Data/{file_name}\",\n","                      repo_id = self.repoID,\n","                      repo_type='dataset',\n","                      token=self.hf_token)\n","      print(f\"Source data {file_name} uploaded into {self.repoID}\")\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def ToRunPipeline(self):\n","    try:\n","      print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","      df_dataset = self.LoadDatasetFromHF()\n","      if df_dataset is None:\n","        return False\n","      else:\n","        df_train, df_test = self.TrainTestSplit(df_dataset)\n","        if df_train is None or df_test is None:\n","          return False\n","        else:\n","          df_train_cleaned = self.DatasetCleaning(df_train)\n","          df_test_cleaned = self.DatasetCleaning(df_test)\n","          if df_train is None or df_test is None:\n","            return False\n","          else:\n","            result_train = self.UploadIntoHF(df_train_cleaned,\n","                                             self.Subfolders,'train.csv')\n","            result_test = self.UploadIntoHF(df_test_cleaned,\n","                                            self.subfolders,'test.csv')\n","            if not result_train or not result_test:\n","              print('Splitted dataset upload into HF Exception')\n","              return False\n","            else:\n","              print('Dataset downloaded from HF, Cleaned, Splitted into train and test dataset and uploaded back into HF dataset')\n","              return True\n","    except Exception as ex:\n","      print(f\"Exception message in ToRunPipeline: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)"],"metadata":{"id":"aFDYFew4-GjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.Model Building with Experimentation Tracking"],"metadata":{"id":"vpSiNqjnWIf_"}},{"cell_type":"code","source":["#%%writefile $base_path/BuildingModels.py\n","\n","import os\n","import joblib\n","import inspect\n","import traceback\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from xgboost import XGBClassifier\n","from datasets import load_dataset\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.impute import SimpleImputer\n","from huggingface_hub.utils import RepositoryNotFoundError\n","from huggingface_hub import HfApi, create_repo, login\n","from huggingface_hub import hf_hub_download\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import precision_recall_curve, precision_score\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.metrics import recall_score, f1_score, classification_report\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","\n","class BuildingModels:\n","  def __init__(self,base_path, hf_token=None):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    self.models = {}\n","    self.best_model = None\n","    self.best_score = 0\n","    self.best_f1_score =0.0\n","    self.best_model_threshold = 0.0\n","    self.best_model_name=None\n","    self.df_train = pd.DataFrame()\n","    self.df_test = pd.DataFrame()\n","    self.feature_train = pd.DataFrame()\n","    self.feature_test = pd.DataFrame()\n","    self.target_train = pd.Series()\n","    self.target_test = pd.Series()\n","    self.base_path = base_path\n","    self.Subfolders = os.path.join(base_path,'data')\n","    self.repo_id = 'jpkarthikeyan/Tourism_Prediction_Model'\n","    self.ds_repo_id = 'jpkarthikeyan/Tourism-visit-with-us-dataset'\n","    self.repo_type = 'model'\n","    self.hf_token = hf_token\n","    self.categorical_columns = ['TypeofContact','Occupation','Gender','ProductPitched','MaritalStatus','Designation']\n","    self.numerical_columns = ['Age','CityTier','DurationOfPitch','NumberOfPersonVisiting',\n","                              'NumberOfFollowups','PreferredPropertyStar',\n","                              'NumberOfTrips','Passport','PitchSatisfactionScore','OwnCar',\n","                              'NumberOfChildrenVisiting','MonthlyIncome']\n","\n","    self.pipeline_numerical = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='median')),\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    self.pipeline_onehot = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore',sparse_output=False))\n","    ])\n","\n","  def Load_data_from_HF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      print(f'Loading the train dataset from {self.ds_repo_id}')\n","\n","      self.df_train = pd.read_csv(hf_hub_download(\n","                                repo_id = self.ds_repo_id,\n","                                filename = 'Master/Data/train.csv',repo_type='dataset'))\n","      self.df_test = pd.read_csv(hf_hub_download(\n","                                repo_id = self.ds_repo_id,\n","                                filename = 'Master/Data/test.csv',repo_type='dataset'))\n","      print(f\"Shape of the train dataset: {self.df_train.shape}\")\n","      print(f\"Shape of the train dataset: {self.df_test.shape}\")\n","\n","      return True\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def Preprocessing_dataset(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","\n","      self.target_train = self.df_train['ProdTaken']\n","      self.feature_train = self.df_train.drop(['ProdTaken'],axis=1)\n","\n","      self.target_test = self.df_test['ProdTaken']\n","      self.feature_test = self.df_test.drop(['ProdTaken'],axis=1)\n","\n","      return True\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def Building_Models(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      preprocessor = ColumnTransformer(\n","          transformers=[\n","              ('num', self.pipeline_numerical,self.numerical_columns),\n","              ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore',\n","                        sparse_output=False),self.categorical_columns)])\n","      models_params = {\n","          'DecisionTreeClassifier':{\n","              'model': DecisionTreeClassifier(class_weight='balanced',random_state=42),\n","              'params': {'classifier__criterion':['gini','entropy'],\n","                         'classifier__splitter':['best','random'],\n","                        'classifier__max_depth':[1],\n","                         'classifier__min_samples_leaf':[1,2,4],\n","                         'classifier__min_samples_split':[2,5,10],\n","                         'classifier__max_features':['sqrt','log2',None]}\n","          },\n","\n","          'RandomForestClassifier':{\n","              'model': RandomForestClassifier(class_weight='balanced',random_state=42),\n","              'params': { 'classifier__n_estimators':[25,50,75,100],\n","                          'classifier__criterion':['gini','entropy'],\n","                          'classifier__max_depth':[5,10,15],\n","                          'classifier__min_samples_split':[15,20,25],\n","                          'classifier__min_samples_leaf':[7,10,15],\n","                          'classifier__max_features':[0.3,0.5,0.6],\n","                          'classifier__oob_score':[True],\n","                          'classifier__bootstrap':[True]\n","                         }\n","          },\n","\n","          'BaggingClassifier':{\n","              'model': BaggingClassifier(estimator=DecisionTreeClassifier(class_weight='balanced',random_state=42)),\n","              'params':{  'classifier__n_estimators':[10,50,75,100],\n","                          'classifier__max_samples':[0.3,0.5,0.7,0.9],\n","                          'classifier__max_features':[0.3,0.5,0.7],\n","                          'classifier__oob_score':[True],\n","                          'classifier__estimator__criterion':['gini','entropy'],\n","                          'classifier__estimator__max_depth':[5,7,9],\n","                          'classifier__estimator__min_samples_split':[8,10,12],\n","                          'classifier__estimator__min_samples_leaf':[2,3,5]\n","                        }\n","          },\n","\n","          'AdaBoostingClassifier':{\n","              'model': AdaBoostClassifier(random_state=42),\n","              'params':{  'classifier__n_estimators':[50,75,100],\n","                          'classifier__learning_rate':[0.01,0.05,0.1],\n","                          'classifier__algorithm':['SAMME','SAMME.R']\n","\n","                      }\n","          },\n","\n","          'GradientBoostingClassifier':{\n","              'model': GradientBoostingClassifier(random_state=42),\n","              'params':{\n","                          'classifier__n_estimators':[50,75,100,125],\n","                          'classifier__learning_rate':[0.01,0.5,0.1],\n","                          'classifier__criterion':['friedman_mse','squared_error'],\n","                          'classifier__max_features':['sqrt','log2'],\n","                          'classifier__min_samples_leaf':[1,2,4],\n","                          'classifier__subsample':[0.6,0.7,0.8],\n","                          'classifier__max_depth':[2,3,4,5]\n","                        }\n","          },\n","\n","          'XGBoostingClassifier':{\n","              'model':XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n","              'params':{'classifier__n_estimators':np.arange(50,100,10),\n","                        'classifier__max_depth': [3,5,7],\n","                        'classifier__learning_rate':[0.01,0.1,0.2],\n","                        'classifier__subsample':[0.6,0.8,1.0],\n","                        'classifier__colsample_bytree':[0.6,0.8,1.0],\n","                        'classifier__gamma':[0,1,2],\n","                        'classifier__reg_alpha':[0,1,2]\n","\n","                        }\n","          }\n","\n","        }\n","\n","      cv_KFold = KFold(n_splits=3,random_state=42,shuffle=True)\n","\n","      for model_name, mdl_params in models_params.items():\n","        print(f'Model {model_name} started')\n","        pipeline = Pipeline(steps=[\n","            ('preprocessor',preprocessor),\n","            ('classifier',mdl_params['model'])\n","        ])\n","        random_search = RandomizedSearchCV(pipeline,mdl_params['params'],\n","                                           n_iter=50,cv=cv_KFold,scoring='f1',\n","                                           random_state=42,n_jobs=-1,verbose=2)\n","\n","        random_search.fit(self.feature_train,self.target_train)\n","\n","        self.models[model_name] = {\n","            'model':random_search.best_estimator_,\n","            'best_score': random_search.best_score_,\n","            'best_params':random_search.best_params_\n","        }\n","        joblib.dump(random_search.best_estimator_,f'{self.base_path}/Model_Dump_JOBLIB/{model_name}.joblib')\n","        print(f'model:{random_search.best_estimator_}')\n","        print(f'best_score: {random_search.best_score_}')\n","        print(f'best_params: {random_search.best_params_}')\n","        print(f'Modle {model_name} completed')\n","        print('-'*50)\n","\n","      return self.models\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","    finally:\n","      print('-'*50)\n","\n","  def Model_Evaluation(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    df_metrics = pd.DataFrame()\n","    try:\n","      for mdl_name, mdl_info in self.models.items():\n","        model = mdl_info['model']\n","        predict_proability = model.predict_proba(self.feature_test)\n","        print(f\"Predict proability shape {mdl_name} {predict_proability.shape}\")\n","        if predict_proability.shape[1] ==1:\n","          predict_proability = predict_proability.flatten()\n","        else:\n","          predict_proability = predict_proability[:,1]\n","\n","\n","        prc_precision,prc_recall, prc_threshold = precision_recall_curve(self.target_test,predict_proability)\n","        prc_f1score = 2*((prc_precision*prc_recall) / (prc_precision+prc_recall+1e-10))\n","\n","        prc_threshold_idmx = np.argmax(prc_f1score)\n","        prc_best_threshold = prc_threshold[prc_threshold_idmx]\n","        print(f'best threshold: {prc_best_threshold}')\n","\n","        predic_prob_threshold = (predict_proability >= prc_best_threshold).astype(int)\n","        #predic_prob_threshold = (predict_proability >= 0.5).astype(int)\n","        accuracy = accuracy_score(self.target_test,predic_prob_threshold)\n","        precision = precision_score(self.target_test,predic_prob_threshold)\n","        recall = recall_score(self.target_test,predic_prob_threshold)\n","        f1score = f1_score(self.target_test,predic_prob_threshold)\n","        class_report = classification_report(self.target_test,predic_prob_threshold)\n","        conf_matrix = confusion_matrix(self.target_test,predic_prob_threshold)\n","\n","        lbl = ['TN', 'FP', 'FN', 'TP']\n","        cnf_lbl = ['\\n{0:0.0f}'.format(cnf_val) for cnf_val in conf_matrix.flatten()]\n","        cn_percentage = [\"\\n{0:.2%}\".format(item/conf_matrix.flatten().sum()) for item in conf_matrix.flatten()]\n","\n","        confusion_label = np.asarray([[\"\\n {0:0.0f}\".format(item)+\"\\n{0:.2%}\".format(item/conf_matrix.flatten().sum())]\n","                                for item in conf_matrix.flatten()]).reshape(2,2)\n","\n","        cnf_label = np.asarray([f'{lbl1} {lbl2} {lbl3}' for lbl1, lbl2, lbl3 in zip(lbl, cnf_lbl,  cn_percentage)]).reshape(2,2)\n","\n","        plt.figure(figsize = (3,3))\n","        sns.heatmap(conf_matrix, annot = cnf_label, cmap = 'Spectral', fmt='' )\n","        plt.xlabel('Predicted')\n","        plt.ylabel('Actual')\n","        plt.title(f'{mdl_name} confusion matrix')\n","        plt.tight_layout()\n","        plt.show()\n","\n","        df_metrics = pd.concat([df_metrics,pd.DataFrame({'model':[mdl_name],'accuracy':[accuracy],\n","                                            'precision':[precision], 'recall':[recall],\n","                                            'f1_score':[f1score]})],ignore_index=True)\n","\n","        if f1score > self.best_f1_score:\n","          self.best_f1_score = f1score\n","          self.best_model_threshold = prc_best_threshold\n","          self.best_model_name = mdl_name\n","\n","      best_model = self.models[self.best_model_name]['model']\n","      if hasattr(best_model, 'feature_importances_'):\n","        feature_importance = pd.DataFrame({\n","            'feature':self.feature_train.columns,\n","            'importance': best_model.feature_importances_\n","        }).sort_values('importance',ascending=False)\n","        print('Feature Importance:\\n',feature_importance)\n","\n","\n","      return df_metrics\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","    finally:\n","      print('-'*50)\n","\n","  def Register_BestModel_HF(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      best_model = self.models[self.best_model_name]['model']\n","      joblib.dump(best_model,f'{self.base_path}/Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib')\n","\n","\n","      api = HfApi()\n","      try:\n","        api.repo_info(repo_id=self.repo_id,repo_type=self.repo_type)\n","      except RepositoryNotFoundError:\n","        api.create_repo(repo_id=self.repo_id, repo_type=self.repo_type,private=False)\n","\n","\n","      print(\"Uploading the best model into Hugging face\")\n","      api.upload_file(path_or_fileobj = f'{self.base_path}/Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib',\n","                      path_in_repo = f\"Model_Dump_JOBLIB/BestModel_{self.best_model_name}.joblib\",\n","                      repo_id=self.repo_id, repo_type=self.repo_type\n","                      )\n","\n","\n","      print(\"Uploading the best threshold text file to HF\")\n","      with open('Master/Model_Dump_JOBLIB/best_threshold.txt','w') as f:\n","        f.write(str(self.best_model_threshold))\n","      api.upload_file(path_or_fileobj = f\"{self.base_path}/Model_Dump_JOBLIB/best_threshold.txt\",\n","                      path_in_repo = f\"Model_Dump_JOBLIB/best_threshold.txt\",\n","                      repo_id=self.repo_id, repo_type=self.repo_type\n","                      )\n","\n","      return True\n","\n","\n","    except Exception as ex:\n","      print(f\"Exception: {ex}\")\n","      traceback.print_exc()\n","      return False\n","    finally:\n","      print('-'*50)\n","\n","  def ToRunPipeline(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    try:\n","      if not self.Load_data_from_HF():\n","        return False\n","      else:\n","        if not self.Preprocessing_dataset():\n","          return False\n","        else:\n","          Build_Model = self.Building_Models()\n","          print(Build_Model)\n","          if Build_Model:\n","            df_Metrics = self.Model_Evaluation()\n","            print(df_Metrics)\n","            if not df_Metrics.empty:\n","              if self.Register_BestModel_HF():\n","                return True\n","              else:\n","                return False\n","            else:\n","              return False\n","          else:\n","            return False\n","    except Exception as ex:\n","      print(f'Exception occured {ex}')\n","    finally:\n","      print('-'*50)\n"],"metadata":{"id":"Og90ls1JWWto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Function"],"metadata":{"id":"VmVIs57DX9vR"}},{"cell_type":"code","source":["import os\n","import logging\n","from google.colab import drive\n","from google.colab import userdata\n","\n","def main():\n","\n","  drive.mount('/content/drive/')\n","  %cd '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1/'\n","  base_path = 'Master/'\n","  print(f\"Base Path {base_path}\")\n","\n","  data_path = os.path.join(base_path, 'Data')\n","  model_joblib_path = os.path.join(base_path, 'Model_Dump_JOBLIB')\n","\n","  print(\"checking for Program folder created or not\")\n","  if not os.path.exists(base_path):\n","    os.makedirs(base_path, exist_ok=True)\n","  else:\n","    print(f\"folder already exists {base_path}\")\n","\n","  if not os.path.exists(data_path):\n","    os.makedirs(data_path, exist_ok=True)\n","  print(f\"folder already exists {data_path}\")\n","\n","  if not os.path.exists(model_joblib_path):\n","    os.makedirs(model_joblib_path, exist_ok=True)\n","  print(f\"folder already exists {model_joblib_path}\")\n","\n","  try:\n","    hf_token = userdata.get('HF_Token')\n","  except Exception as ex:\n","    print(f\"Exception: {ex}\")\n","    raise ValueError(f\"HF TOKEN not found in the colab secrets\")\n","\n","  # DATA REGISTRATION\n","  try:\n","    ObjDataReg = DataRegistration(base_path,hf_token)\n","    if not ObjDataReg.ToRunPipeline():\n","      print(\"Exception Data Registration\")\n","      return False\n","    else:\n","      print(\"Data Registration Completed\")\n","      return True\n","  except Exception as ex:\n","    logging.error(f\"Exception {ex}\")\n","\n","  # DATA PREPRATION\n","  try:\n","    ObjDataPrep = DataPrepration(base_path,hf_token)\n","    if not ObjDataPrep.ToRunPipeline():\n","      logging.error(\"Data Registration failed\")\n","      return False\n","    else:\n","      logging.info(\"Data Registration Completed\")\n","      return True\n","  except Exception as ex:\n","    logging.error(f\"Exception {ex}\")\n","    return False\n","\n","\n","  # MODEL BUILDING\n","  try:\n","    ObjMdlBuild = BuildingModels(base_path,hf_token)\n","    if not ObjMdlBuild.ToRunPipeline():\n","      print(\"Exception Model Building\")\n","      return False\n","    else:\n","      print(\"Model Building Completed\")\n","      return True\n","  except Exception as ex:\n","    logging.error(f\"Exception {ex}\")\n","    return False\n","\n","\n","\n","if __name__ == \"__main__\":\n","  main()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6FBhO071yyd","executionInfo":{"status":"ok","timestamp":1755246821083,"user_tz":-330,"elapsed":4775,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"b8178636-a266-431e-ddaa-b6b1f7d74973"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1\n","Base Path Master/\n","checking for Program folder created or not\n","folder already exists Master/\n","folder already exists Master/Data\n","folder already exists Master/Model_Dump_JOBLIB\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Exception name 'DataRegistration' is not defined\n","ERROR:root:Exception name 'DataPrepration' is not defined\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSgi2U5F21Eb","executionInfo":{"status":"ok","timestamp":1755246828194,"user_tz":-330,"elapsed":133,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"2a3a05ed-4c37-4f4e-92fa-b23e968f1d7a"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism_version_1_1\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AI6b4wCq3G7M","executionInfo":{"status":"ok","timestamp":1755246830620,"user_tz":-330,"elapsed":108,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"3d01dd2d-b3ee-43bd-bdb3-95b13559236e"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Master\n"]}]},{"cell_type":"code","source":["%%writefile .github/workflows/pipeline.yml\n","name: VISIT WITH US TOURISM PREDICTION PIPELINE\n","\n","on:\n","  push:\n","    branches: [ main ]\n","  pull_request:\n","    branches: [ main ]\n","\n","jobs:\n","  data_registration:\n","    runs-on: ubuntu-latest\n","    steps:\n","    - name: Checkout Repository\n","      uses: actions/checkout@v4\n","\n","    - name: Set up Python\n","      uses: actions/setup-python@v4\n","      with:\n","        python-version: '3.12'\n","\n","    - name: Install Dependencies\n","      run: |\n","        python -m pip install --upgrade pip\n","        pip install jupyter nbconvert huggingface_hub pandas\n","\n","    - name: SET UP FOLDER STRUCTURE\n","      run: |\n","        mkdir -p VisitWithUs-Tourism_version_1_1/Master/Data\n","        mkdir -p VisitWithUs-Tourism_version_1_1/Master/Model_Dump_JOBLIB\n","        if [ -f \"tourism.csv\"]; then\n","          cp tourism.csv VisitWithUs-Tourism_version_1_1/Master/Data/tourism.csv\n","        else\n","          echo \"Error: tourism.csv not found\"\n","          exit 1\n","        fi\n","\n","    - name: CONVERT NOTEBOOK TO PYTHON SCRIPT\n","      run: |\n","        jupyter nbconvert --to python Visit-With-Us-Tourism-Prediction_v1_1.ipynb --output pipeline_script\n","\n","    - name: RUN PIPELINE SCRIPT FOR DATA REGISTRATION\n","      env:\n","        HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","      run: |\n","        echo \"import logging\" > run_data_registration.py\n","        echo \"logging.basicConfig(level=logging.INFO, format='%(asctime)s -%(levelname)s -%(message)s' , filename ='data_registration.log')\" >> run_data_registration.py\n","        echo \"logging.info('Starting DataRegistation')\" >> run_data_registration.py\n","        echo \"from DataRegistation import DataRegistration\" >> run_data_registration.py\n","        echo \"base_path = 'VisitWithUs-Tourism_version_1_1/Master/'\" >> run_data_registration.py\n","        echo \"dr = DataRegistration(base_path=base_path,hf_token='$HF_TOKEN')\" >> run_data_registration.py\n","        echo \"dr.ToRunPipeline()\" >> run_data_registration.py\n","        echo \"logging.info('DataRegistration failed')\" >> run_data_registration.py\n","        echo \"exit 1\" >> run_data_registration.py\n","        python run_data_registration.py\n","\n","    - name: Upload logs and script\n","      if: always()\n","      uses: actions/upload-artifact@v4\n","      with:\n","        name: data-registration-outputs\n","        path: |\n","          data_registration.log\n","          run_data_registration.py"],"metadata":{"id":"WjL8ua--5jUx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755246861778,"user_tz":-330,"elapsed":103,"user":{"displayName":"Karthikeyan JP","userId":"08094035270994931409"}},"outputId":"6b1332dd-13f2-460a-d20e-00ef5685554a"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing .github/workflows/pipeline.yml\n"]}]},{"cell_type":"markdown","source":["#Front End Implementation"],"metadata":{"id":"-PWTuTZHusYj"}},{"cell_type":"code","source":["pip install streamlit"],"metadata":{"id":"IppdmKSyu3G-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/requirements.txt\n","pandas\n","numpy\n","scikit-learn\n","joblib\n","streamlit\n","huggingface_hub\n","xgboost"],"metadata":{"id":"dW90Nwy5rQE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/README.md\n","---\n","title: Visit With Us - Tourism package prediction\n","emoji: 🚩\n","colorFrom: blue\n","colorTo: green\n","sdk: docker\n","sdk_version: 3.9\n","app_file: app.py\n","app_type: streamlit\n","pinned: false\n","license: mit\n","---\n","The streamlit app predicts the customer will purchace the tourism package"],"metadata":{"id":"mrU_4ijQrl_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/Dockerfile\n","# Use a minimal base image with Python 3.9 installed\n","FROM python:3.9-slim\n","\n","# Set the working directory inside the container to /app\n","WORKDIR /app\n","\n","# Copy all files from the current directory on the host to the container's /app directory\n","COPY . .\n","\n","# Install Python dependencies listed in requirements.txt\n","RUN pip install --no-cache-dir -r requirements.txt\n","RUN mkdir -p /tmp/hf_cache && chmod -R 777 /tmp/hf_cache\n","ENV HF_HOME=/tmp/hf_cache\n","ENV HUGGINGFACE_HUB_CACHE=/tmp/hf_cache\n","ENV PYTHONUNBUFFERED=1\n","\n","\n","EXPOSE 7860\n","\n","\n","# Define the command to run the Streamlit app on port \"7860\" and make it accessible externally\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=7860\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"],"metadata":{"id":"F96pJ8MK1jPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile Master/Deployment/app.py\n","import streamlit as st\n","import pandas as pd\n","import joblib\n","import os\n","import logging\n","from huggingface_hub import login,hf_hub_download\n","from xgboost import XGBClassifier\n","#from google.colab import userdata\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","cache_dir = \"/tmp/hf_cache\"\n","os.environ[\"HF_HOME\"] = cache_dir\n","os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_dir\n","\n","try:\n","  hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n","\n","  if hf_token:\n","    login(token=hf_token)\n","    logger.info(\"Successfully logged in to Hugging Face\")\n","  else:\n","    logger.error(\"Hugging face token not found\")\n","    st.error(\"Huggingface token not found\")\n","except Exception as ex:\n","  logger.error(f\"Failed to login to Hugging face: {ex} \")\n","  st.write(f\"Failed to login to Hugging face: {ex} \")\n","\n","try:\n","  os.makedirs(cache_dir, exist_ok=True)\n","  logger.info(f\"Created cache directory {cache_dir}\")\n","except Exception as ex:\n","  logger.error(f\"Failed to create cache directory {cache_dir}: {ex}\")\n","  st.error(f\"Failed to create cache directory {cache_dir}: {ex}\")\n","\n","\n","st.title(\"Visit with Us: Tourism Package Prediction\")\n","st.write(\"Enter the Customer details to predict the likehood of purchasing the tourism packages\")\n","\n","\n","if 'predictor' not in st.session_state:\n","  st.session_state.predictor = None\n","  st.session_state.model_loaded = False\n","\n","class PredictorTourism:\n","\n","  def __init__(self):\n","    self.Subfolders = 'Master'\n","    self.repoID = 'jpkarthikeyan/Tourism_Prediction_Model'\n","    self.model = None\n","    self.best_threshold = 0.0\n","\n","  def Load_Model(self):\n","    try:\n","      logger.info(\"Loading best model\")\n","      model_path = hf_hub_download(\n","          repo_id = self.repoID,filename = f'Model_Dump_JOBLIB/BestModel_XGBoostingClassifier.joblib',\n","          repo_type = 'model')\n","      threshold_path = hf_hub_download(\n","          repo_id = self.repoID, filename=f'Model_Dump_JOBLIB/best_threshold.txt',\n","          repo_type='model')\n","\n","      logger.info(f\"Model path: {model_path}\")\n","      logger.info(f\"Threshold path:  {threshold_path}\")\n","\n","      self.model = joblib.load(model_path)\n","      with open(threshold_path,'r') as f:\n","        self.best_threshold = float(f.read())\n","      st.success(\"Model and threshold loaded successfully\")\n","      return True\n","\n","    except Exception as ex:\n","      st.error(f'Exception: {ex}')\n","      return False\n","\n","\n","  def Predict(self, data):\n","    try:\n","      logger.info(f\"Input Data: {data}\")\n","      df= pd.DataFrame([data])\n","      logger.info(f\"Data shape: {df.shape}\")\n","      logger.info(f\"Dataframe columns: {df.columns.tolist()}\")\n","      prob = self.model.predict_proba(df)[:,1]\n","      prediction = int(prob >= self.best_threshold)\n","      return prediction\n","\n","    except Exception as ex:\n","      logger.error(f\"Exception in predict: {ex}\", exc_info=True)\n","      st.error(f\"Exception Prediction: {ex} {traceback.print_exc}\")\n","      return ex\n","\n","\n","if not st.session_state.model_loaded:\n","  st.session_state.predictor = PredictorTourism()\n","  st.session_state.model_loaded = st.session_state.predictor.Load_Model()\n","\n","with st.form(\"customer_form\"):\n","  st.header(\"Customer Details\")\n","  col1, col2,col3 = st.columns(3)\n","\n","  with col1:\n","\n","    age = st.number_input(\"Age\", min_value=18, max_value=100, value=41)\n","    gender = st.selectbox('Gender',['Male','Female'])\n","    MaritalStatus = st.selectbox('MaritalStatus',['Married','Unmarried','Single','Divorced'])\n","    Occupation = st.selectbox('Occupation',['Free Lancer','Salaried','Small Business','Large Business'])\n","    Designation = st.selectbox('Designation',['AVP','Manager','Executive','Senior Manager','VP'])\n","    MonthlyIncome = st.number_input('MonthlyIncome',min_value=0, max_value=1000000,value=20999)\n","\n","  with col2:\n","\n","    typeofcontact = st.selectbox(\"TypeofContact\",['Self Enquiry','Company Invited'])\n","    citytier = st.selectbox('citytier',[1,2,3], index=2)\n","    DurationOfPitch = st.number_input('DurationOfPitch', min_value=1, max_value=60, value=6)\n","    ProductPitched = st.selectbox('ProductPitched',['Deluxe','Basic','Standard','Super Deluxe','King'])\n","    PreferredPropertyStar = st.selectbox(\"'PreferredPropertyStar\",[3,2,1])\n","    NumberOfTrips = st.number_input('NumberOfTrips',min_value=0, max_value=30, value=1)\n","\n","\n","  with col3:\n","    NumberOfPersonVisiting = st.number_input('NumberOfPersonVisiting',min_value=1,max_value=10,value=3)\n","    NumberOfFollowups = st.number_input('NumberOfFollowups',min_value=0,max_value=10, value=3)\n","    NumberOfChildrenVisiting= st.number_input('NumberOfChildrenVisiting',min_value=0,max_value=5,value=0)\n","    Passport= st.selectbox('Passport',['Yes','No'],format_func=lambda x:\"Yes\" if x==\"Yes\" else \"No\")\n","    Owncar= st.selectbox('OwnCar',['Yes','No'],format_func=lambda x:\"Yes\" if x==\"Yes\" else \"No\")\n","    PitchSatisfactionScore= st.number_input('PitchSatisfactionScore',min_value=1,max_value=5,value=3)\n","\n","\n","  submitted = st.form_submit_button(\"Predict\")\n","\n","if submitted:\n","  input_data = {\n","      'Age':age,\n","      'TypeofContact':typeofcontact,\n","      'CityTier':citytier,\n","      'DurationOfPitch':DurationOfPitch,\n","      'Occupation':Occupation,\n","      'Gender':gender,\n","      'NumberOfPersonVisiting':NumberOfPersonVisiting,\n","      'NumberOfFollowups':NumberOfFollowups,\n","      'ProductPitched':ProductPitched,\n","      'PreferredPropertyStar':PreferredPropertyStar,\n","      'MaritalStatus':MaritalStatus,\n","      'NumberOfTrips':NumberOfTrips,\n","      'Passport':1 if Passport ==\"Yes\" else 0,\n","      'OwnCar':1 if Owncar ==\"Yes\" else 0,\n","      'PitchSatisfactionScore':PitchSatisfactionScore,\n","      'NumberOfChildrenVisiting':NumberOfChildrenVisiting,\n","      'Designation':Designation,\n","      'MonthlyIncome':MonthlyIncome\n","\n","  }\n","\n","\n","  if st.session_state.predictor:\n","    result = st.session_state.predictor.Predict(input_data)\n","\n","    if result is not None:\n","      st.subheader(f\"Prediction Result is {result}\")\n","      st.write(f\"Likely to purchase\" if result ==1 else \"Unlikely to purchase\")\n","    else:\n","      st.write(result)\n","      st.error(\"Error in prediction\")\n","  else:\n","    st.error(\"Models are not loaded, please ensure the model and threshold are available on Hugging face\")\n","\n"],"metadata":{"id":"MgAoOlhJvV_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Hosting:\n","  def HostingHFSpace(self):\n","    print(f\"Function Name {inspect.currentframe().f_code.co_name}\")\n","    api = HfApi()\n","    repo_id = 'jpkarthikeyan/Tourism-Prediction-Model-Space'\n","    directory_to_upload = '/content/drive/MyDrive/PGP_AI_ML_GREAT_LEARNING/10_Advance_Machine_Learning_And_MLOps/Final_Project/VisitWithUs-Tourism/Master/'\n","\n","    try:\n","      api.repo_info(repo_id = repo_id, repo_type='space')\n","      print(f\"Space {'repo_id'} already existis\")\n","    except RepositoryNotFoundError:\n","      create_repo(repo_id= repo_id, repo_type='space',\n","                       space_sdk= 'docker', private = False)\n","      print(f\"Space created {repo_id}\")\n","\n","    api.upload_folder(repo_id = repo_id, folder_path = f'{directory_to_upload}/Deployment/',\n","                      repo_type='space')\n","\n","\n","if __name__ == '__main__':\n","  ObjSpace = Hosting()\n","  ObjSpace.HostingHFSpace()"],"metadata":{"id":"x_puqsQWuv-C"},"execution_count":null,"outputs":[]}]}